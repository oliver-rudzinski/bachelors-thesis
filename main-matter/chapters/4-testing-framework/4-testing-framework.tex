%===========================================================================
%	IV. Testing Framework Design
%===========================================================================

The creation of a DataOps testing framework aims to assure the data and software quality requirements of a solution. In general, based on the findings from Section \ref{sec:2-testing}, the following design process of such a framework is proposed:

\begin{enumerate}
	\item Include data quality checks within the solution at hand based on pre-defined data requirements or data governance specifications.
	\item Define test data suites that invoke the data quality checks, resulting in both positive and negative outcomes.
	\item Create automated tests for the solution that make use of the test data suites to verify that the data quality and solution performance requirements are met. These tests will run whenever a new version of the solution is about to be deployed into production.
\end{enumerate}

This chapter takes these steps, elaborates on them, and applies the project's \ac{mba} data pipeline solution on the process. It aims to provide a guideline for designing DataOps testing based on a given scenario. For the desired pilot demonstration, the definition of a testing framework for one specific stage within the data pipeline is sufficient. The Conversion Stage of the \ac{mba} data pipeline is suited for this task since it prepares the Athena database integration of raw \ac{pos}Log data. This includes a variety of data transformation and processing, and thus, is prone to data quality issues throughout the entire flow of the stage.

\section{Data Quality Check Design}
In order to check for data quality issues within a data flow, it is important to understand the different steps the given solution performs during its flow. As stated in Section \ref{sec:2-2-value-pipeline}, this also includes the inspection of the raw input data and the final processed output data. In a multi-stage environment, it is also important to consider the required input for the next pipeline stages, in this case, the Sanitization Stage.

\subsection{Conversion Stage Data Governance Review}
Input and output data requirements should be defined by the use case's data governance. In the Conversion Stage's case, the input data should consist of one or multiple \ac{pos}Log \ac{xml} files (cf. Section \ref{sec:3-3-conversion}). The input data is also expected to be named in a unified format which contains retail branch store information, date information, as well as the identification number of the receipt (cf. Section \ref{sec:3-2-data-lake}). The data needs to originate from the \texttt{raw} subarea within the specific analytics job area inside the data lake (cf. Figure \ref{fig:3-data-lake}) The output data needs to be provided in \ac{json} file format, where one \ac{pos}Log \ac{xml} file yields one or multiple \ac{json} \ac{pos} files, containing global receipt as well as single-purchase information (cf. Section \ref{sec:3-3-conversion}), depending on the number of items from the source \ac{pos}Log file. Thus, the data values of the individual files do not change but get rather split up and reformatted. For the output data, a similar unified naming format, including the order of purchases from the original \ac{pos}Log file, is also required. The data destination is the \texttt{prepared} sub-area within the specific analytics job area inside the data lake. This data governance information can now be summarized in Table \ref{tab:4-data-gov-convert}:

\begin{table}[h!]
	\centering
	\begin{tabular}{r!{\vrule width 1pt}l!{\vrule width 0.5pt}l}
\textbf{}                & \textbf{Input Data}                                                                                           & \textbf{Output Data}                                                                                                                        \\ \ChangeRT{1pt}
\makecell[r]{\textbf{Data}\\\textbf{Format}}     & \makecell[l]{\texttt{\ac{xml}}\\with \ac{pos}Log formatting}                                                                          & \makecell[l]{\texttt{\ac{json}}\\with defined single-\ac{pos} formatting}                                                                                    \\ \ChangeRT{0.5pt}
\makecell[r]{\textbf{Data}\\\textbf{Amount}}     & \makecell[l]{$n > 0$}                                                                                     & \makecell[l]{$m \cdot n \geq n$ \\ where $m > 0$ is the number of individual \\ items for each \ac{pos}Log file $1$ to $n$}                          \\ \hline

\makecell[r]{\textbf{File}\\\textbf{Naming}}     & \makecell[l]{\texttt{\textless{}store-id\textgreater{}\_\textless{}date\textgreater{}\_} \\ \texttt{pos\textless{}no\textgreater{}.xml}} & \makecell[l]{\texttt{\textless{}store-id\textgreater{}\_\textless{}date\textgreater{}\_}\\ \texttt{pos\textless{}no\textgreater{}\_\textless{}sq\textgreater{}.json}} \\ \hline


\makecell[r]{\textbf{Location}\\\textbf{Format}} & \makecell[l]{\texttt{s3://\textless{}bucket-name\textgreater{}/} \\ \texttt{\textless{}job-id\textgreater{}/raw}}                        & \makecell[l]{\texttt{s3://\textless{}bucket-name\textgreater{}/} \\ \texttt{\textless{}job-id\textgreater{}/prepared}}                  
\end{tabular}
	\caption{Conversion Stage Data Governance Specification Summary}
	\label{tab:4-data-gov-convert}
\end{table}

The corresponding format examples have already been provided in Source Code Excerpt \ref{src:3-poslog} and Source Code Excerpt \ref{src:3-json}.

As previously depicted in Figure \ref{fig:3-convert}, the Conversion Stage runs through the following data-focussed tasks to achieve the conversion:

\begin{enumerate}
	\item Take the data from the \ac{s3} bucket and read its (binary) contents.
	\item Map those contents to \texttt{OrderedDict}s using Python's \texttt{xmltodict} library
	\item Remove nesting from the dictionaries by reformatting each \texttt{OrderedDict} to multiple, single-\ac{pos} standard Python dictionaries
	\item Export each single-\ac{pos} dictionary to \ac{json} (binary) format
	\item Upload the files to the \ac{s3} bucket
\end{enumerate}

Each task is now prone to different sorts of data quality issues, each of those violating one or multiple \ac{dama} dimensions of data quality, mentioned in Section \ref{sec:2-2-value-pipeline}: \textit{Completeness} issues appear, when data which was actually saved by the retail register is missing during analysis. Since this project is based on pseudorandomly generated data, this dimension falls out of the scope of the testing framework. A lack of \textit{Uniqueness} happens when the analytics engine processes multiple identical \ac{pos}Log files. This check should be moved to the testing framework of the next stage since distinction checks are simpler to perform within \ac{sql}. In case the data is otherwise valid, \textit{Timeliness} problems can be found when the timestamp of the data file name does not correspond to the timestamp inside the \ac{pos}Log file. The Conversion Stage is mostly prone to \textit{Validity} issues, including the format of the input and output data, the schema of the corresponding files or specific values inside the files or intermediate transformation results. \textit{Accuracy}, again, falls out of scope since technical issues with a real-life register system are not covered inside this project.  \textit{Consistency} in a retail \ac{bi} solution is ambivalent, since product discounts, names, etc. can become updated over time.

For the sake of simplicity during implementation, data timeliness aspects are also treated inside the scope of data validity. For the Conversion Stage, this results in the data quality categories \textit{Data Format Quality}, \textit{Data Schema Quality}, and \textit{Data Value Quality}.

\subsection{Data Event Definition Process}
The various potential data quality issues need to be defined in a unified format, creating a task sheet for implementation. These definitions are referred to as \textit{Data Events} and include the following information:

\begin{description}
	\item[Event Description] Detailed information on the event
	\item[Event Category] Category of the data event
	\item[Severity] Extent of influence of the data event
	\item[Handing] Detailed information on the handling of the data event
\end{description}

The degree of severity is important, since not every data quality issue has the same level of influence on the performance of the system. A corrupted file, which cannot be processed, should have a high severity. On the other hand, an exclusively incorrect file naming could have a minor impact on the analysis, but should not require a process termination. These severities need to be defined and justified individually based on the given use case, but a general classification of severity degrees could be useful. In the following, the severity levels based on a specification by Oracle \cite{oracle} are used:

\begin{description}
	\item[\texttt{INFO}] An informative message, usually describing task activity. No action is necessary (for logging purposes only).
	\item[\texttt{WARNING}] Minor derivation from expected performance. \textit{Might} cause analytical mistakes when appearing more often.
	\item[\texttt{ERROR}] Major derivation from expected performance, but still recoverable. \textit{Will} cause analytical mistakes when appearing more often.
	\item[\texttt{CRITICAL}] Severe derivation from expected performance that cannot be recovered from. Causes immediate task termination.
\end{description}

The \texttt{WARNING} and \texttt{ERROR} levels imply the implementation of thresholds that may reach a maximum appearance value for each level. Since errors are more severe than warnings by logic, the threshold for errors should also be stricter than the one for warnings.

\subsection{Conversion Stage Input Data Events} \label{sec:4-1-events}
The data event definition process is now applied to the Conversion Stage. For redundancy reduction, the following events are chosen to provide the most distinct data event cases. The lift of all derived data events for the Conversion Stage can be found in Appendix \ref{app:data-events}.
\newpage
\subsubsection{Example A: Data Source Empty} \label{sec:4-1-3-a}
\begin{table}[h!]
\centering
\begin{tabular}{r!{\vrule width 1pt}l}
\makecell[r]{\textbf{Description}} & \textbf{No data inside the designated data lake area} \\ \ChangeRT{1pt}
\makecell[r]{\textbf{Category}}    & Data Format                         \\ \ChangeRT{0.5pt}
\makecell[r]{\textbf{Severity}}    & \texttt{CRITICAL}                               \\ \hline
\makecell[r]{\textbf{Handling}}    & \makecell[l]{
	1. Log error including data lake location information \\
	2. Prompt error information \\
	3. Terminate analytical process \\
}   
\end{tabular}       
	\caption{Data Event Example A: Data Source Empty}
\end{table}

At the beginning of the Conversion Stage, the previously unzipped \ac{xml} files need to be available for access. If, for any reason, the provided data source is empty, the process needs to terminate immediately, since the next steps (and, ultimately, the \ac{mba} itself) could not be performed without any data. Therefore, there is no recovery possible, which is why the process is terminated. The information on the empty \ac{s3} bucket is prompted and logged. This data event mostly applies to the data format issue category. \\\


\subsubsection{Example B: \acs{xml} File(s) Corrupted} \label{sec:4-1-3-b}
\begin{table}[h!]
\centering
\begin{tabular}{r!{\vrule width 1pt}l}
\makecell[r]{\textbf{Description}} & \textbf{One or multiple XML file(s) corrupted} \\ \ChangeRT{1pt}
\makecell[r]{\textbf{Category}}    & Data Format                         \\ \ChangeRT{0.5pt}
\makecell[r]{\textbf{Severity}}    & \texttt{ERROR}                               \\ \hline
\makecell[r]{\textbf{Handling}}    & \makecell[l]{
	1. Remove corrupted file from analysis \\
	2. Increase \texttt{ERROR} degree counter \\
	3. Calculate threshold difference, terminate if exceeded \\
	4. Flag file (\texttt{xmlCorr-err}) \\
	5. Log error including file information \\
	6. Continue analytical process \\
}                                   
\end{tabular}

	\caption{Data Event Example B: \acs{xml} File(s) Corrupted}
	\label{tab:4-xml-corrupted}
\end{table}

After the data has been recognized, it needs to run through a process of checking the file validity. This does not take the actual content of the \ac{xml} files into account but rather checks if the content is processable. If, for any reason, one or multiple source files are corrupted, they cannot be processed by means of the following steps. This represents an event with \texttt{ERROR} severity. Depending on the volume of source data, this data might be neglected and removed from the current analysis, or the analysis needs to be terminated because the threshold for \texttt{ERROR}-throwing events has been exceeded. A file tag is added to the source file that caused the event invocation for troubleshooting purposes. This data event also applies to the data format issue category. This event definition can be similarly applied to empty data files or files with incorrect headers (i.e., non-\ac{pos}Log files) because of their comparable effects on the analytics performance. \\\


\subsubsection{Example C: Missing Optional Attributes} \label{sec:4-1-3-c}
\begin{table}[h!]
\centering
\begin{tabular}{r!{\vrule width 1pt}l}
\makecell[r]{\textbf{Description}} & \textbf{One or multiple XML files is missing \textit{Optional Attributes}} \\ \ChangeRT{1pt}
\makecell[r]{\textbf{Category}}    & Data Schema                         \\ \ChangeRT{0.5pt}
\makecell[r]{\textbf{Severity}}    & \texttt{WARNING}                               \\ \hline
\makecell[r]{\textbf{Handling}}    & \makecell[l]{
	1. Increase \texttt{WARNING} degree counter accordingly \\
	2. Calculate threshold difference, terminate if exceeded \\
	3. Flag file (\texttt{optArg-warn}) \\
	4. Log warning including file information \\
	5. Continue analytical process \\
}           
\end{tabular}
	\caption{Data Event Example C: Missing Optional Attributes}
\end{table}

After the data has been validated for processing, its content needs to be evaluated as well. Considering the upcoming Sanitization Stage, it requires valid values for its \texttt{SequenceNumber}, \texttt{ItemCategory}, \texttt{ItemID}, as well as the \texttt{Quantity} attribute (cf. Source Code Excerpt \ref{src:3-athena-query}), hereinafter referred to as \textit{Required Attributes}. All remaining attributes (i.e., \textit{Optional Attributes}) are not relevant for the analysis but are still desired to be valid to rule out any unforeseen performance issues. Since the data is still processable by means of the current and upcoming pipeline stages, this data event is classified to have a rather minor impact on the overall outcome. Nevertheless, it should not occur too often since this could point to other issues inside the source data. Therefore, the \texttt{WARNING} event severity is applied here. In case its threshold is exceeded, the analysis will be terminated, similar to the \texttt{ERROR} event threshold. Again, the source data receives a file tag. This data event applies to the data schema issue category. Furthermore, a data event that covers the corresponding value of such an attribute would be categorized as a data value event. This event definition can be similarly applied to a \textit{data naming} or \textit{data extension} issue since it has a similarly low degree of severity for the analytics outcome. \\\

Each further data event should be defined in such a manner that the severity of it is evaluated based on the data governance which can then propose handling measures for the implementation. It is also important to consider the order of the performed data quality checks. For example, checking for required arguments inside a file cannot happen before a data corruption check.

\subsection{Conversion Stage Transformation and Output Data Events} \label{sec:4-1-4}
The same procedure needs to be applied for transformation and output data within the specific stage. This might include similar or different threshold and event severity definitions. In the case of the Conversion Stage, the transformation of the data is purely focussed on its formatting. This means that the outcome for each further step is deterministic. If an error occurs during the transformation, this implies software-side issues within the solution that need to be tested and ruled out individually. From a data perspective, the input data has been validated prior to the first transformation step and, thus, cannot be the reason for incorrect, further performance.

This means that all upcoming data events resulting in errors during transformation or within output data need to be treated with \texttt{CRITICAL} severity level, resulting in instant termination once such an error occurs. This measure can help detect an underlying source code problem in the transformation steps during analysis.

In other cases (e.g. \acs{ml} model training, which is not deterministic), the previous paragraph does not apply. Based on the use case and its circumstances, the handling of errors of all sorts need to be evaluated and defined appropriately.

\section{Test Data Design} \label{sec:4-test-data-design}
The next step is to create applicable test data suites in accordance to the data event definitions. During the DataOps Testing process, these data will be ingested into the analytics stage, determining if the data events are recognized and handled by the solution as expected. In general, the test data should remain as close to real production-grade data as possible, which also means the inclusion of production-grade values inside the \ac{pos}Log file (i.e., actual product descriptions, prices, realistic quantities, etc.).

\subsection{Single-Event Test Data} \label{sec:4-single-event}
At first, it might be reasonable to check if all data event handling works individually. This means considering each defined data event as a single test case. The preparation of applicable test data can also be seen as the intentional violation of the data governance for each data event.

Remaining inside the Conversion Stage, Example A (cf. Section \ref{sec:4-1-3-a}) should cover the occurrence of an empty data source. This can be tested by providing an empty job sub-area within a testing environment inside the data lake. It is expected that this case will trigger the event handling, resulting in process termination. For Example B (cf. Section \ref{sec:4-1-3-b}), the appropriate trigger is a (binarily) corrupted \ac{xml} file. The solution is expected to check for content validity. Such a corrupted data piece is expected to be removed from the current (test) analysis process. Finally, Example C (cf. Section \ref{sec:4-1-3-c}) requires a valid \ac{pos}Log \ac{xml} file with an arbitrary missing optional attribute. Inside this stage of testing, it is important that this is the only issue inside the test data file since the individual data event handling performance should be evaluated first. Otherwise, another data event might unexpectedly be triggered. A missing optional attribute is expected to be flagged by the system, to log the warning, to calculate the threshold tolerance, etc.

It might also be useful to provide a generally clean and valid data piece in order to check that no issue event is triggered when no issues are present.

\subsection{Multi-Event Test Data} \label{sec:4-multi-event}
Increasing complexity, the next step in defining test data is to combine several data issues in a single file. Based on an expected (i.e., desired) outcome, this might be useful for checking if the order of data validity checks is kept.

Inside the Conversion Stage, this might be the combination of missing both required and optional attributes. In case a required attribute is missing, the data file should be taken out of the analysis (cf. Appendix \ref{app:data-events}). The missing optional attribute will only trigger a data flag. If, for any reason, the data is not removed, this might point to misimplementation of the data handling mechanisms. Still, each test data piece is considered its own test case.

\subsection{Test Data Suites} \label{sec:4-test-suites}
After the individual test cases have been run through, it might be reasonable to include a (close-to) production-grade scenario inside the testing framework. This means that a testing suite of multiple test data files is created. The test data should be designed in a manner that it contains both correct and incorrect data. The incorrect data within the suite should also include different degrees of faultiness. This is expected to enable testing of the thresholding capability of the solution as well as general handling performance when dealing with multiple data files.

Given the Conversion Stage, approx. half of the data could be correct and valid. One fourth of the data might contain minor issues (e.g., incorrect file naming, missing optional arguments, etc.). The remaining fourth of the data might contain major issues (e.g., corrupted or empty files, missing required arguments, etc.). The predefined threshold values should be taken into account when designing such a test data suite.


\section{DataOps Solution Testing Design} \label{sec:4-3-testing-architecture}
The final step of the DataOps Testing framework design is to validate the functionality of the data event handling and the general solution by means of automated tests through the application of the test data suites. This makes use of the classical software testing paradigm (cf. Section \ref{sec:2-2-innovation-pipeline}):

\begin{itemize}
	\item Unit tests will cover all functionality outside of the data scope of the solution. This might include the setting of correct environment variables within the solution, validation of required credentials, etc.
	\item  Integration tests will be used for the data handling validation. This includes the integration of the external data lake, containing the test data suites. On the one hand, the tests will ensure that the appropriate data handling measures are taken at the correct position within the source code. On the other hand, the tests will also validate that the data handling measures actually conform to the data event definitions.
	\item  End-to-end tests will be built on top of the integration tests, running through the entire analysis process and taking all external infrastructure into account.
\end{itemize}
\subsection{Conversion Stage Testing Architecture}
\subsubsection{Conversion Stage Unit Testing}
Inside the Conversion Stage, unit testing does not play a major role. This is because the majority of tasks performed by the stage are data-orientated. Prior to the analytics performance, the stage evaluates the environment variables of the system that are expected to contain the \ac{uri} paths to the data source and data destination within the \ac{s3} data lake. A similar measure happens for the validation of resource access credentials. These processes are subject to unit testing.

\subsubsection{Conversion Stage Integration (Data) Testing}
Integration tests play the most important role inside the testing framework. Since all data (production-grade data as well as test data suites, for the sake of centralization) reside on the external \ac{s3} data lake, the interplay with this infrastructure component needs to be taken into account during testing. Integration testing within this solution can be divided into two parts: First, each data event is evaluated individually based on its predefined single-event test data (cf. Section \ref{sec:4-single-event}). In this part, the tests validate that each data issue is \textit{recognized} and \textit{handled} in its appropriate context. When a corrupted \ac{xml} file is provided, this needs to be recognized (i.e., logged and reported) as well as handled (i.e., corrupted data is removed from analysis).

When this validation is conducted successfully, the multi-event test data (cf. Section \ref{sec:4-multi-event} can be induced for similar test cases. At this point, the general functionality of the individual data handling measures can be expected. Finally, the test data suite(s) (cf. Section \ref{sec:4-test-suites}) are used to test the thresholding capability of the solution, which is not testable with single-file test cases.

\subsubsection{Conversion Stage End-To-End Testing}
The final testing stage inside the Conversion Stage should also take all remaining infrastructure into account. In this case, this includes Airflow, which should be triggered by the end-to-end test and receive the test data suites. Airflow runs through the entire data pipeline and reports its production-like outcome which is then evaluated by the end-to-end test. This will ensure that no dependencies have been left out during testing.

\subsection{Application of DataOps Testing}
The testing framework is applied whenever a new version of the analytics solution is about to be deployed into production. In case a new feature is developed or an existing feature is updated or removed, the tests ensure that the core analytics capacity is maintained. Without these tests, version dependencies or classical software bugs could be deployed into production, resulting in analytics result issues or unresolvable crashes of the system. In case one of the test cases does not pass, the deployment is not performed which allows the correction of the underlying process. This procedure is repeated until all tests have passed. In case of the development of entirely new features or data handling measurements, applicable test cases need to be created to validate these features, as well. This includes regression testing, taking requirements from remaining features and other stages into account. \\\

Again, this testing framework design process needs to be individually applied to the context of the given use case. Other solutions might require more extensive unit testing or have a more complex end-to-end testing structure. These requirements need to be evaluated prior to test design and implementation.

The testing implementation process itself is even more individual. This is because different ways of implementation might lead to the same outcome. The implementation of the testing framework for the Conversion Stage will be described in the next chapter.
