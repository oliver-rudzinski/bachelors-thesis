In general, software development relies on testing principles to holistically and objectively validate the expected performance of a piece of software. Neglecting testing in the discipline of software engineering might lead to customers and stakeholders of the solution loosing trust in its integrity and the increased occurrence of bugs and issues. In data analytics, the urge for testing is even stronger since errors in analytics outcomes might not only slow down business processes but have an enormous negative impact on data-reliant business decisions. An analytics solution might not crash and seemingly perform as expected but its resulting reports could contain errors that cannot be identified easily. DataOps-driven solutions also fall under this category, which is why these need to be ultimately tested for both software performance and data integrity.

To provide fundamental information on the testing principles required for designing a DataOps use case solution, the following sections define the integrity requirements of data analytics solutions (Section \ref{sec:2-1-requirements}), describe applicable frameworks and processes of both software and data testing, and eventually combine these by means of the DataOps methodology.



\subsection{Integrity Requirements of Data Analytics Solutions} \label{sec:2-1-requirements}
In order to define integrity requirements, the following section focusses on why results of data analytics processes are important, why bad data might impact these results, and what measures need to be taken into account for the solution to provide valuable analysis results.

Nowadays, organizations depend on data analytics more than ever \cite{Munawar2011}. \ac{bi} and \ac{dwh} solutions are designed to utilize data for business-required decision making \cite{Souibgui2019}. While these systems are expected to generate value, many companies lose trust in their data analytics because it might be prone to unforeseeable errors \cite{BISurvey.com}. This is because \ac{bi} and \ac{dwh} systems rely on high-quality data in order to provide representative analytics results and business insights \cite{Munawar2011}. Unfortunately, data quality issues of various sorts and manifestations lead to the systems generating false and potentially misleading reports \cite{Freudiger2014}\cite{Munawar2011}\cite{Redman2016}. Trying to reverse-engineer the preliminary data problems based on faulty analytics outcomes is not a valid approach. Instead, data quality is already required at its source to allow for efficient analytics and decision-making \cite{Redman2020}\cite{Tricentis}. Data quality assessment becomes increasingly harder due to an exponential rise in overall data volume in the past \cite{Freudiger2014}. Plus, complex analytics solutions are more fault-prone, which makes it harder to assess and find potential issues in their performance \cite{Homayouni2018}.

Addressing data quality issues needs to be a priority when performing analytics \cite{Freudiger2014}\cite{Veber2018}\cite{Redman2020}. Expecting blanket existence of data quality and, thus, not addressing it inside of the data analytics solution lifecycle, is a primary mistake \cite{Munawar2011}. Instead, data quality needs to be continuously assessed in each stage of such a solution. This includes the validation of the input data, testing for potential errors during data transformation, and making sure the output data complies with its origin \cite{Homayouni2018}\cite{Munawar2011}\cite{Redman2016}. Moreover, when multi-stage data analytics processes are utilized, the potential sources of errors is also multiplied, which requires individual testing of input, transformation, and output of each stage of the solution \cite{Homayouni2018}.

Data quality assessment needs not only to be performed for recently generated or required data, but for the entire database in use \cite{Redman2020}\cite{Freudiger2014}. Historic data is generally more prone to inconsistencies than recently acquired data since the former often originate from legacy systems, where infrastructure migrations might have lead to further quality issues \cite{Sneed2013}. This is especially crucial with \acs{ml}-driven applications since historical data is used for model training. If a model does not represent a specific situation correctly, its application for recent analyses will be inconclusive as well \cite{Redman2018}. Continuous monitoring of data quality issues within the utilization of \ac{bi} solutions could prove helpful for incremental improvement of outcomes \cite{Munawar2011}. Automation of the mentioned testing and validation processes increases productivity even more by reducing human error, accelerating detection, and introducing recovery measures within the conducted analyses \cite{Homayouni2018}\cite{Veber2018}.

All in all, data analytics solutions require tests regarding data quality at every stage where data is introduced, transformed, or processes. Furthermore, the software needs to report recognized issues and fall back into recovery protocols, if applicable. The automation of these measures is desired for better efficiency. Since data analytics solutions remain software product, the plain functionality needs to be assessed as well by means of traditional software testing and enriched with respect to data quality. 

\subsection{Software Testing}
Software testing is the process of analyzing the behavior of software, primarily to detect anomalies and defects that could be categorized as a software \textit{bug}. Software testing is conducted under controlled conditions and includes both positive and negative presets and a defined expected outcome for each testing use case \cite{Mahfuz2016}.

Software testing consists of various methods, types, and levels. The following section covers the relevant aspects of software testing for designing a testing concept within a data analytics solution.

\subsubsection{Testing Methods: Black Box vs. White Box Testing}
Testing methods are applied to decide the degree of abstraction of a test case. In general, either the \textit{Black Box} or the \textit{White Box} testing method is chosen. Black box testing is applied on test cases with a high degree of abstraction. The conducted test verifies the basic functionality of the application without considering its specific internal code logic. Therefore, creating such a test does not require deep knowledge of the specific code structure. The test literally treats the test candidate as a black box and checks if, given a specific input, a specific expected output is achieved \cite[65]{Mahfuz2016}

On the other hand, white box testing takes the internal code logic and structure into account. Its goal is to cover all possibilities of outcome, meaning to reach as many statements, condition checks and paths of the test candidate as possible. This implies that the test can only be written with good knowledge of the test candidate's source code operation. By conducting white box testing, not only expected behaviors are assessed by means of their final results. Instead, this testing method aims to confirm the internal integrity of the test candidate \cite[65]{Mahfuz2016}.

In general, both white-box and black-box testing can be leveraged for any level of software testing. In practice, black-box testing is more often used on higher levels of testing, as the internal integrity is expected to be tested on the lower levels, where white-box testing is rather utilized \cite[26]{Ammann2017}.

Introducing agility to testing is often seen as a testing method on its own \cite[70]{Mahfuz2016}. Since DataOps is conducted in an overall agile manner, non-agile methodologies of testing are not taken into consideration within this work.

\subsubsection{Testing Levels} \label{sec:2-2-2-levels}
Levels within software testing define the magnitude in which a test case is conducted \cite{Osherove2013}. These levels range from testing individual, granular software components in a highly controlled and isolated environment to global test runs of the overall behavior of the solution in its actual deployment environment \cite{Mahfuz2016}

\paragraph{Unit Testing} Unit testing is defined as the lowest level of software testing \cite[65]{Mahfuz2016}. As the name implies, the corresponding tests take individual components (i.e., \textit{units}) from the source code and check their behavior. Unit tests are pieces of code that invoke the chosen test unit and compare its actual result with the expected outcome \cite[sec. 1]{Osherove2013}. Depending on the test purpose, unit tests apply black box testing for basic functionality checks and white box testing for deeper internal functionality assessment. This requires detailed knowledge of the code under test \cite{Mahfuz2016}. Because of their granularity, a large number of unit tests is often required to test the software sufficiently. Thus, unit tests need to be of low complexity. They also need to be automated and repeatable, which requires a highly isolated test run environment. In general, unit test runs must perform idempotently. Each test needs to be able to run independently and cannot rely on outcomes of previous unit test runs. The unit test should reflect the intended usage of the test candidate, which also means that it needs to be relevant for the future of the code under test \cite[sec. 2]{Osherove2013}.

\paragraph{Integration Testing} Integration testing builds up on unit testing \cite[66]{Mahfuz2016}. Performing integration tests verifies that multiple combined units are working together as expected. It focusses on the testing of interfaces that connect singular components \cite[66]{Mahfuz2016} and is seen as an important counterpart to unit testing \cite[sec. 3]{Osherove2013}, since it covers areas that are usually not taken into account when performing isolated unit tests. Due to more dependencies outside of a controlled testing environment, the results of such tests might not be always consistent \cite[sec. 3]{Osherove2013} which implies the need for more abstract testing. It is also important that integration test only provide meaningful insights on the operation of the tested software when unit tests have been conducted and passed successfully. This is also emphasized in so-called \textit{incremental integration testing}. This process reflects the software development lifecycle by taking the addition of new features and components into account. The principle requires that the new or updated components are independently able to perform correctly before checking their integration in a higher-level testing combination \cite[66]{Mahfuz2016}.

\paragraph{End-to-End Testing} End-to-end testing is the highest level of software testing \cite[67]{Mahfuz2016} Usually, \textit{System Testing} is described as a level between integration and end-to-end testing. Since the underlying project is presented as a fundamental proof of concept for DataOps and contains a fairly simple system architecture, system tests do not lie inside the scope of this work. Aspects of system testing are also embodied inside end-to-end testing since it aims to be performed under (close-to) deployment requirements. This creates an idealized real-life scenario which makes use of external environments. Thus, end-to-end testing is the least isolated level and serves as the final test stage before actual deployment. Usually, this includes a run-through from start to finish \cite[67]{Mahfuz2016}. Again, end-to-end testing can only provide insightful measures about the integrity of a system when the lower test levels have passed successfully.

\subsubsection{Testing Types}
There are several testing types in software testing to further categorize the purpose of tests. Testing types either classify a subset of testing processes or are applied on top of them to increase its meaningfulness.

\paragraph{Functional Testing}
Functional testing verifies that software is created in compliance with its pre-defined, functional requirements \cite[69]{Mahfuz2016}. All traditional testing levels fall under the category of functional testing since they assess the general functionality of software and do not consider specific measures of how this functionality is achieved.

\paragraph{Non-Functional Testing}
Non-functional testing is the counterpart of functional testing \cite[69]{Mahfuz2016}. It verifies that the software achieves measures defined as non-functional requirements. This includes but is not limited to load, stress, or performance testing. While these terms are often used to describe similar activities \cite[70]{Mahfuz2016}, they have slightly different meanings. Load testing describes assessing the software's limitations based on heavy loads. Stress testing goes above these limits and does also take other factors of \textit{stress} into account (e.g., repetition of similar or identical processes, etc.). Performance tests are used for the long-term analysis of performance decrease over time \cite[70]{Mahfuz2016}. Other non-functional testing types include security and user experience. Since these aspects are not covered in this work, they are not further discussed here.

\paragraph{Regression Testing}
Regression testing is one of the most common testing types \cite[70]{Mahfuz2016} and can be found on each level of software testing \cite{Mathur2013}. It aims to uncover errors with pre-existing components when other components were newly included, changed, or removed. In other words, each part of the software must still perform correctly when a part of the features changed \cite[70]{Mahfuz2016}\cite{Mathur2013}. These tests are especially required when new releases are about to be deployed into production. Generally, regression tests are conducted for both corrective and progressive reasons, meaning that they can uncover current and potential future regression issues. To achieve regression testing in practice, unit tests can be written in a more abstract manner and re-run for the entire software solution, requiring older unit tests still to pass \cite{Mathur2013}. Thus, regression tests also highly benefit from automation \cite{Mahfuz2016}.

\paragraph{Smoke Testing}
Smoke testing describes a subset of test cases on several testing levels that assesses crucial, basic functionality of software. Smoke tests not passing can be an indicator for the core features of a (piece of) software not performing properly. They also imply that deploying the failing state of the software will most likely result in unrecoverable crashes. Using smoke tests allows for quick recognition of fundamental errors before performing other, much more granular tests \cite[sec. 5]{Tarlinder2016}.

\subsection{Data Quality Testing} \label{sec:2-2-3-data-testing}
The reasoning behind data quality testing is explained in the same manner as software testing: It ensures that the result of an analysis will be conducted properly and as expected as well as that problems are caught before they can cause any harm. In general, testing the data dimension of a data analytics solution ensures data quality \cite[2]{Savanur2016}. Other than with software testing, data quality testing does not rely on pre-defined and well-documented methods. Instead, the quality of the data in question needs to be purposefully defined based on its area of application \cite[1]{Askham2013}\cite{Schieferdecker2012}\cite[116\psq]{Sneed2013}\cite[667]{Souibgui2019}. This type of data governance provides rules \cite{Schieferdecker2012}\cite[116\psq]{Sneed2013} which can then be embedded inside the data analytics solution. 

\subsubsection{Dimensions of Data Quality}
Nevertheless, there are certain dimensions of data quality that are applied on the process of defining such rules. The \ac{dama} in the United Kingdom defined \enquote{The Six Primary Dimensions for Data Quality Assessment} in 2013 \cite[7\psqq]{Askham2013}. This section introduces these dimensions which will be used for defining data quality measures for the practical use case at hand at a later point of this thesis.

\paragraph{Completeness}
Data completeness describes the proportion of the stored data against the potential of being one-hundred percent complete \cite[8]{Askham2013}\cite{Shen2019}. It requires that there exists a definition for the \textit{whole} completeness of the data inside the given use case \cite[8]{Askham2013}. This is because an application might require certain data items and treat others as optional. An example for the lack of data completeness might be a personal data assessment form where the last name of a person is missing, even though the corresponding input field was marked required to be filled.

\paragraph{Uniqueness}
While completeness asks for (nearly) full data existence coverage, the data \textit{uniqueness} dimension requires it not to be more than that. Uniqueness is achieved when each unique data record only exists once inside the entire database at hand \cite[9]{Askham2013}. Several sources point out that duplicate data is a primary reason for misleading analyses, which means that achieving data uniqueness should be a top priority \cite{Shen2019}\cite[677\psq]{Souibgui2019}. An example for the lack of data uniqueness could be an employee database table listing 520 employees, even though only 500 people work at the given company.

\paragraph{Timeliness}
The timeliness of data is the degree to which data represents the reality from the required point in time. Whether a specific data record (or series of data records) is timely or not depends on the corresponding use case and data governance definition. This also includes the time difference of the creation of the data and its actual usage \cite[10]{Askham2013}. Depending on a use case, timeliness could be defined very strictly (e.g., for analyzing trends in stock trading markets) or rather loosely (e.g., changing the primary contact address of a person).

\paragraph{Validity}
Data validity describes a data item corresponding to its expected (and therefore, pre-defined) format, schema, syntax, etc. This definition should also include a range of expected or acceptable variation thresholds \cite[11]{Askham2013}. Testing for data schematics is one processes which allows for definitive objective differentiation between good and bad data \cite{Schieferdecker2012}. When a certain data item does not comply with its expected standards, it can be considered bad. Data validity provides metrics that can be included in the analytics process to efficiently rule out data quality issues without having thorough knowledge of the meaning of the data at hand \cite{Shen2019}.

\paragraph{Accuracy}
Data accuracy is the degree to which data \textit{correctly} describes the actual object or event existing in the real world. Defining data characteristics for accuracy can often be a long-term process which requires in-depth knowledge of the represented area of application \cite[9]{Askham2013}. Data accuracy goes hand-in-hand with data timeliness since outdated data can be a primary cause for bad data accuracy. Lack of accuracy might especially occur with invalid data entries that cannot be checked for \cite[12]{Askham2013}. One example might lay in the different date formats in the world. A user might presume the European data format (\texttt{DD-MM-YYYY}), even though the system requires U.S. date format (\texttt{MM-DD-YYYY}). As long as the input does not exceed the expected values, a syntactically correct but inaccurate data record is created.

\paragraph{Consistency}
Data consistency describes the absence of difference when comparing multiple representations of the same real-life object against its actual definition \cite[13]{Askham2013}. In other words, two occurrences of the same object do not differ if the object itself did not change between those occurrences. An example for data inconsistency might be two data records of identical purchases made by different customers where they are being charged different total amounts (discounts excluded).

\\\

These core dimensions can be enriched by taking other, more specific factors into consideration, e.g., data usability, data confidence, data value \cite[13\psq]{Askham2013}. It is also notable that the weighing of the respective dimensions strictly relies on the data analytics application \cite[5]{Askham2013}. This means that \textit{Timeliness} might be of great importance in one project, but could be neglected in another. Apart from that, certain dimensions \textit{cannot} be achieved without the existence of others, and other dimensions \textit{might} be achieved even if others are not.

\subsubsection{Data Quality Testing in Practice}
As mentioned inside the data quality dimension definitions, areas of data quality are mostly achieved when data complies with the corresponding data governance definitions. These allow for measuring the data quality \cite{Schieferdecker2012}. 

By including corresponding checks into the analytics software, not valuable data can be handled accordingly. These data quality checks are then performed before, during, and after the respective analysis. These checks must be designed in the same manner that can be seen in software testing: Either (a set of) data is good and passes the checks, or it it bad and fails \cite[1]{Savanur2016}, which then leads to analysis recovery measures, a program exit, etc. Additionally, data profiling \cite{GartnerGlossary3} could be used to fix recognizable and correctable issues. This could especially be useful with minor data quality inconsistencies, which might only cause problems when occurring in an extensive frequency. Plus, depending on the analytics context, some issues could even be neglected \cite[678\psq]{Souibgui2019}. Especially with data quality testing in multi-stage analytics pipeline environments, internal data lineage tracing is crucial in order to find out at which point the data failed its tests \cite{Shen2019}. Log files can then be used to reverse-engineer the data quality issue \cite{Askham2019}.

\\\

In conclusion, including  quality checks based on pre-defined data governance regulations, data quality issues could be recognized and handled accordingly. This could reduce the amount and severity of analytics reports resulting from bad data.

\subsection{Test Quality Measurement and Evaluation}
An important factor for testing, both for traditional software as well as data analytics solutions, is to measure the quality of the respective software and data tests \cite[30]{Brader2012}.

\subsubsection{Software Testing: Test Code Coverage}
A popular method for defining test quality in software development is to check how much code was executed during the run of a particular test suite \cite[30]{Brader2012}. It is expected that code with a high percentage of test code coverage will be less likely to contain unexpected faults. Microsoft suggests a code coverage of 80 percent or above for production-grade solutions \cite[30]{Brader2012}. On the other hand, high test coverage does not guarantee quality in code \cite[8]{Heckman2014}. This is because the execution of a code piece might have been invoked but the test suite did not perform any tests on this specific fragment. It is up to the developer to create tests that are not only aiming for a high coverage as the primary subject, but to consider all possible paths throughout the code under test \cite[9]{Heckman2014}.

There exist multiple categories of coverage that can point out lack of testing in certain areas. Based on the testing level, they can give insights on a specific code file, a specific area inside the code base or the entire code base in question.

\begin{description}
	\item[Line Coverage] defines the percentage of lines that have been called and executed without failure during the run of a test suite.
	\item[Statement Coverage] defines the percentage of statements that have been called and executed without failure during the run of a test suite \cite[2]{Heckman2014}. Other than with line coverage, a multi-line statement (e.g., definition and initialization of a large array, formatted for better readability) is considered singularly, which could improve the precision of the coverage.
	\item[Branch Coverage] checks if all paths of the code under test have been executed. This is important for conditions (i.e., \texttt{if/else if/else} statements) that need to be called in different scenarios to ensure coverage on the majority of execution cases \cite[2]{Heckman2014}.
	\item[Function/Method Coverage] represents the percentage of functions or methods that have been called and executed without failure during the run of a test suite \cite[2]{Heckman2014}. This process does not consider the importance or length of the functions or methods but can be used as an indicator if certain areas of the code have been left out of the testing suite entirely.
\end{description}
	
\subsection{Data Testing: Requirements Compliance}
As the findings of data testing in general (cf. Section \ref{sec:2-2-3-data-testing}) suggest, measurement of data testing quality strongly depends on the data governance requirements. All in all, it can be suggested that test cases should be designed to enforce tests inside crucial parts of the data definition, while optional or less application-areas could be treated less strictly. As with software test code coverage, a large number of data checks that inspect a large area of the data under test may not necessarily guarantee error-free data. It solely verifies that the tested areas will not make the analytics solution break or abort its processes. Moreover, the coverage is not expected to verify that analytics reports based on the allegedly clean data will be correct and insightful. The definition of high-quality data test cases is—again—up to the developer.
	

\subsection{Testing in DataOps}
As described previously in Section \ref{?}\todo{Add DataOps pipelines section}, DataOps contains a two-dimensional pipeline methodology. The Value Pipeline is used for the actual, real-time, production-grade analytics while the Innovation Pipeline continuously allows for feature updates of the analytics solution. The goal of DataOps testing is now to ensure correct and expected functionality of both dimensions \cite[40\psqq]{Bergh2019}. It is now to take the findings from the previous sections and to deduct an abstract, high-level framework for DataOps testing.

In general, the \enquote{Duality of testing in DataOps} \cite[40]{Bergh2019} now also requires two dimensions of tests which considers one of the two pipelines fixed, respectively. When testing the Value Pipeline, the underlying source code of the solution is fixed while various sorts of data are expected to flow through the pipeline. On the other hand, testing the Innovation Pipeline means checking for potential errors inside the analytics solution's update beyond its data. Here, the data is fixed (thus, pre-defined) and used to check whether the updated source code is ready to be deployed into production \cite[40]{Bergh2019}. 

\subsubsection{Value Pipeline: Data Quality Testing}
Various data flows through the Value Pipeline continuously. Traditionally, its goal is to take presumably valuable input data, perform various forms of data transformation and analysis, and report its output to some kind of knowledge base. With the findings on data quality testing (cf. Section \ref{sec:2-2-3-data-testing}), input data should not be presumed \textit{valid}. Now, another important purpose of the Value Pipeline is to conduct applicable data tests and enable measures if incorrect data is recognized. This process needs to be applied at each point where data is ingested, integrated, transformed or derived \cite{Layton2019}\cite{DataKitchen2020}. During the pipeline's performance, the source code (i.e., its features and modus operandi) do not change which is why it is the data stream that needs to be continuously verified. Enabling logging could allow for insightful operations processes finding the cause of the data problem.

\subsubsection{Innovation Pipeline: Software Testing}
The process within the Innovation Pipeline means the development and preparation of new features that are expected to improve the behavior of the analytics solution. Here, the validity of the new update needs to be validated by means of traditional software testing. In this case, using production-grade input data might not ensure the proper performance of the new solution. 

This is why the input data is prepared, taking various cases of data quality (based on the use case definition) into account. This set of test data should include \textit{bad data} which checks if the rejection and recovery measures of the new solution are in place. On the other hand, \textit{good data} needs to be provided \cite[234\psq]{Held2012} which will pass all data-related checks inside the source code and move on to the actual transformation and analysis steps, which then create output data that can be used for the analysis performance evaluation. In general, it is recommended to prepare generally applicable data based on the defined data governance specification rather than picking random data (with, thus, random errors) \cite[115]{Sneed2013}.

Plus, regression testing is a very important factor inside the Innovation Pipeline \cite{Shen2019} since data sets generated prior to the analytics solution update still need to be processable by the new release. Therefore, in case the update is based on a data format change, historic test data should also be included in the test data set. This can be seen as a parallel to pure software regression testing where previously designed unit tests are kept to ensure that a feature update does not collide with unchanged features. However, in case of a drastic feature change, the abstraction or complete removal of tests might be reasonable, depending on the architecture and specific feature update at hand.

\\\

In conclusion, holistic testing inside a DataOps environment is achieved by taking the change of both source code and data into account. The source code should contain a number of data quality checks based on its data governance specification. This approach of data quality testing is expected to allow for confidence in the analytics solution. On the other hand, the source code needs to be tested by means of traditional software testing methods, including the validation of the data quality checks and of all transformation and analysis actions. This is to be supported by an extensive test data repository which needs to be designed based on the production-grade data specifications. These tests are expected to invoke the majority of outcome possibilities, leading to a high test coverage and correct performance validation \cite{DataKitchen2020a}.

This testing duality concept is used for the design of the specific use case testing framework for this thesis' project in Chapter \ref{chap:dataops-testing}.

